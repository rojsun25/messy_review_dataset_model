{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9faf623-e1ec-49d5-bb1d-27cdac7cf946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "# CHANGED: Remove unused pandas and BeautifulSoup import at top\n",
    "# import pandas as pd\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, trim, regexp_replace, size, split, array_distinct, when\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class ReviewDataset:\n",
    "    def __init__(self, spark, input_folder, raw_folder, output_folder):\n",
    "        self.input_folder = input_folder\n",
    "        self.raw_folder = raw_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.spark = spark\n",
    "\n",
    "    @staticmethod\n",
    "    def get_latest_file_in_folder(folder_path: str, pattern: str = \"*.csv\") -> str:  # CHANGED: fixed HTML-escaped -> to ->\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found in {folder_path} with pattern {pattern}\")\n",
    "        latest_file = max(files, key=os.path.getmtime)\n",
    "        return latest_file\n",
    "\n",
    "    @staticmethod\n",
    "    def move_file_to_folder(src_file_path: str, dest_folder: str) -> str:  # CHANGED: fixed ->\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        filename = os.path.basename(src_file_path)\n",
    "        dest_file_path = os.path.join(dest_folder, filename)\n",
    "        shutil.move(src_file_path, dest_file_path)\n",
    "        print(f\"Moved file {filename} to {dest_folder}\")\n",
    "        return dest_file_path\n",
    "\n",
    "    # CHANGED: Remove BeautifulSoup-based cleaners and Python UDFs entirely. Use Spark SQL functions.\n",
    "    @staticmethod\n",
    "    def remove_html_spark(df, review_col: str = \"review_content\"):\n",
    "        \"\"\"\n",
    "        Remove HTML tags using regexp_replace, then lower + trim.\n",
    "        \"\"\"\n",
    "        cleaned = df.withColumn(\n",
    "            review_col,\n",
    "            trim(lower(regexp_replace(col(review_col), \"<[^>]+>\", \"\")))\n",
    "        )\n",
    "        return cleaned\n",
    "\n",
    "    @staticmethod\n",
    "    def map_sentiment_spark(df, rating_col: str = \"rating\"):\n",
    "        \"\"\"\n",
    "        Map numeric rating to sentiment using Spark SQL when/otherwise.\n",
    "        \"\"\"\n",
    "        rating_d = col(rating_col).cast(\"double\")\n",
    "        df = df.withColumn(\n",
    "            \"sentiment\",\n",
    "            when(rating_d >= 4.0, \"positive\")\n",
    "            .when(rating_d == 3.0, \"neutral\")\n",
    "            .when(rating_d <= 2.0, \"negative\")\n",
    "            .otherwise(\"neutral\")\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_suspicious_reviews_spark(df, review_col: str = \"review_content\", min_word_len: int = 5):\n",
    "        df = df.withColumn(\"word_count\", size(split(col(review_col), \" \")))\n",
    "        df = df.withColumn(\"is_suspicious_length\", col(\"word_count\") < min_word_len)  # CHANGED: fixed < operator\n",
    "        df = df.withColumn(\"distinct_words_count\", size(array_distinct(split(col(review_col), \" \"))))\n",
    "        df = df.withColumn(\"is_suspicious_repetitive\", col(\"distinct_words_count\") == 1)\n",
    "        df = df.withColumn(\"suspicious_review\", col(\"is_suspicious_length\") | col(\"is_suspicious_repetitive\"))\n",
    "        df = df.drop(\"word_count\", \"distinct_words_count\", \"is_suspicious_length\", \"is_suspicious_repetitive\")\n",
    "        return df\n",
    "\n",
    "    def ingest_latest_file(self, input_folder, output_folder):\n",
    "        latest_file = self.get_latest_file_in_folder(input_folder, \"*.csv\")\n",
    "        moved_file = self.move_file_to_folder(latest_file, output_folder)\n",
    "        print(f\"Ingested file is now at: {moved_file}\")\n",
    "        return moved_file\n",
    "\n",
    "    def clean_data(self, raw_folder):\n",
    "        latest_file = self.get_latest_file_in_folder(raw_folder, \"*.csv\")\n",
    "        print(f\"Cleaning latest file: {latest_file}\")\n",
    "        # CHANGED: For local workspace paths, use \"file:\" scheme; for DBFS use \"dbfs:/...\"\n",
    "        # If your path is under /Workspace/..., Spark often needs file: prefix.\n",
    "        read_path = latest_file if latest_file.startswith(\"dbfs:/\") else f\"file:{latest_file}\"\n",
    "\n",
    "        df = self.spark.read.option(\"header\", True).csv(read_path)\n",
    "\n",
    "        # CHANGED: Use Spark SQL functions instead of Python UDFs for cleaning\n",
    "        df = self.remove_html_spark(df, review_col=\"review_content\")\n",
    "\n",
    "        # Drop rows where review_content became null or empty after cleaning\n",
    "        df = df.filter(col(\"review_content\").isNotNull() & (trim(col(\"review_content\")) != \"\"))\n",
    "\n",
    "        # Suspicious review detection stays with Spark functions\n",
    "        df = self.detect_suspicious_reviews_spark(df, review_col=\"review_content\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df):\n",
    "        # CHANGED: Use Spark SQL functions for sentiment mapping\n",
    "        df = self.map_sentiment_spark(df, rating_col=\"rating\")\n",
    "        df_final = df.select(\"review_content\", \"sentiment\")\n",
    "        return df_final\n",
    "\n",
    "    def save_output(self, df, output_folder, output_filename=\"training_dataset.csv\"):\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        # CHANGED: Spark writes a folder when using .csv(). Keep .csv extension if you wish,\n",
    "        # but know it will be a directory containing part files.\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "        print(f\"Output saved to {output_path}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            # Step 1: Ingest latest file (optional)\n",
    "            # self.ingest_latest_file(self.input_folder, self.output_folder)\n",
    "\n",
    "            # Step 2: Clean data\n",
    "            df = self.clean_data(self.raw_folder)\n",
    "            df.printSchema()\n",
    "            # CHANGED: replace display(df) with show() for jobs\n",
    "            df.show(20, truncate=False)\n",
    "\n",
    "            # Step 3: Transform data\n",
    "            df_final = self.transform(df)\n",
    "            df_final.show(20, truncate=False)\n",
    "\n",
    "            # Step 4: Save output\n",
    "            self.save_output(df_final, self.output_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"review_dataset\").getOrCreate()\n",
    "\n",
    "    # CHANGED: Define these paths; they were previously undefined.\n",
    "    # Update these to match your environment. If using DBFS, prefer dbfs:/ paths.\n",
    "    input_file_path = \"/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/input\"\n",
    "    raw_file_path = \"/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/raw\"\n",
    "    output_file_path = \"/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/output\"\n",
    "\n",
    "    task = ReviewDataset(spark, input_file_path, raw_file    task = ReviewDataset(spark, input_file_path, raw_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-12-15 22_40_56",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
