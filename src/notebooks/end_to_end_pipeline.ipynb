{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58c5a6bc-32f7-49e2-9531-308bebbc4fe2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install required libraries"
    }
   },
   "outputs": [],
   "source": [
    "# Initialise libraries\n",
    "!pip install -r \"/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/requirements.txt\"\n",
    "\n",
    "# restart kernal\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba47454a-551d-4702-b5c2-8db40628d3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically retrieve input variables (e.g., file path, target path) for parameterized workflows and reusable functions\n",
    "\n",
    "dbutils.widgets.text(\"input_file_path\", \"\")     # /Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/pre_raw/\n",
    "dbutils.widgets.text(\"raw_file_path\", \"\")       # /Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/raw/\n",
    "dbutils.widgets.text(\"output_file_path\", \"\")    # /Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/output/\n",
    "\n",
    "input_file_path = dbutils.widgets.get(\"input_file_path\")\n",
    "raw_file_path = dbutils.widgets.get(\"raw_file_path\")\n",
    "output_file_path = dbutils.widgets.get(\"output_file_path\")\n",
    "\n",
    "# print variables\n",
    "print(f\"Input file path: {input_file_path}\")\n",
    "print(f\"Raw file path: {raw_file_path}\")\n",
    "print(f\"Output file path: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6933fde8-d271-4e19-a5ff-53b5ebf1ded4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, lower, trim, udf, regexp_replace, size, split, array_distinct, when, expr, DataFrame\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "class ReviewDataset:\n",
    "    \"\"\"\n",
    "    A class to process messy review datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark, input_folder, raw_folder, output_folder):\n",
    "        \"\"\"\n",
    "        Initialize the ReviewDataset class with Spark session and input/output paths.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.input_folder = input_folder\n",
    "        self.raw_folder = raw_folder\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "    @staticmethod\n",
    "    def get_latest_file_in_folder(folder_path: str, pattern: str = \"*.csv\") -> str:  \n",
    "        \"\"\"\n",
    "        Find the latest file in a folder\n",
    "        \"\"\"\n",
    "\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found in {folder_path} with pattern {pattern}\")\n",
    "        latest_file = max(files, key=os.path.getmtime)\n",
    "        return latest_file\n",
    "\n",
    "    @staticmethod\n",
    "    def move_file_to_folder(src_file_path: str, dest_folder: str) -> str: \n",
    "        \"\"\"\n",
    "        Move a file to a folder\n",
    "        \"\"\"\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        filename = os.path.basename(src_file_path)\n",
    "        dest_file_path = os.path.join(dest_folder, filename)\n",
    "        shutil.move(src_file_path, dest_file_path)\n",
    "        print(f\"Moved file {filename} to {dest_folder}\")\n",
    "        return dest_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html_spark(df, review_col: str = \"review_content\"):\n",
    "        \"\"\"\n",
    "        Remove HTML tags and links using a Python UDF with BeautifulSoup.\n",
    "        \"\"\"\n",
    "\n",
    "        def clean_html(text):\n",
    "            if text is None:\n",
    "                return None\n",
    "            soup = BeautifulSoup(text, \"lxml\")\n",
    "            # Remove all links\n",
    "            for a in soup.find_all('a'):\n",
    "                a.decompose()\n",
    "            return soup.get_text(separator=\" \", strip=True).lower().strip()\n",
    "\n",
    "        clean_html_udf = udf(clean_html, StringType())\n",
    "        cleaned = df.withColumn(review_col, clean_html_udf(col(review_col)))\n",
    "        return cleaned\n",
    "\n",
    "    @staticmethod\n",
    "    def map_sentiment_spark(df: DataFrame, rating_col: str = \"rating\"):\n",
    "        \"\"\"\n",
    "        Map numeric rating to sentiment using Spark SQL when/otherwise, using try_cast for tolerance.\n",
    "        \"\"\"\n",
    "        rating_d = expr(f\"try_cast({rating_col} as double)\")\n",
    "        df = df.withColumn(\n",
    "            \"sentiment\",\n",
    "            when(rating_d >= 4.0, \"positive\")\n",
    "            .when(rating_d == 3.0, \"neutral\")\n",
    "            .when(rating_d <= 2.0, \"negative\")\n",
    "            .otherwise(\"neutral\")\n",
    "        )\n",
    "        print(f\"Added sentiment column to DataFrame\")\n",
    "        return df\n",
    "\n",
    "    def ingest_latest_file(self, input_folder, output_folder):\n",
    "        latest_file = self.get_latest_file_in_folder(input_folder, \"*.csv\")\n",
    "        moved_file = self.move_file_to_folder(latest_file, output_folder)\n",
    "        print(f\"Ingested file is now at: {moved_file}\")\n",
    "        return moved_file\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_suspicious_reviews_spark(df: DataFrame, review_col: str = \"review_content\", min_word_len: int = 5):\n",
    "        df = df.withColumn(\"word_count\", size(split(col(review_col), \" \")))\n",
    "        df = df.withColumn(\"is_suspicious_length\", col(\"word_count\") < min_word_len)  # CHANGED: fixed < operator\n",
    "        df = df.withColumn(\"distinct_words_count\", size(array_distinct(split(col(review_col), \" \"))))\n",
    "        df = df.withColumn(\"is_suspicious_repetitive\", col(\"distinct_words_count\") == 1)\n",
    "        df = df.withColumn(\"suspicious_review\", col(\"is_suspicious_length\") | col(\"is_suspicious_repetitive\"))\n",
    "        df = df.drop(\"word_count\", \"distinct_words_count\", \"is_suspicious_length\", \"is_suspicious_repetitive\")\n",
    "        return df\n",
    "\n",
    "    def explode_zip_columns(self, df, cols_to_zip,\n",
    "                            sep=\",\",                       # string or dict: {\"colA\": \",\", \"colB\": \"|\", ...}\n",
    "                            keep_other_cols=True,          # keep non-exploded columns in the output\n",
    "                            trim_tokens=True,              # trim whitespace around tokens\n",
    "                            drop_empty_tokens=True,        # drop empty tokens (\"\")\n",
    "                            mode=\"longest\",                # \"longest\" | \"shortest\" | \"strict\"\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Split multiple string columns by delimiter and explode them *in lockstep*.\n",
    "        The i-th split value from each column is kept aligned on the same output row.\n",
    "\n",
    "        modes:\n",
    "        - \"longest\": keep rows up to the longest list; missing values become NULLs.\n",
    "        - \"shortest\": keep only positions where *all* columns have a value (drop NULLs).\n",
    "        - \"strict\":   keep only input rows where all split arrays have *equal length*.\n",
    "        \"\"\"\n",
    "        # Normalize separators\n",
    "        if isinstance(sep, str):\n",
    "            sep_map = {c: sep for c in cols_to_zip}\n",
    "        else:\n",
    "            # dict provided; default to \",\" if missing\n",
    "            sep_map = {c: sep.get(c, \",\") for c in cols_to_zip}\n",
    "\n",
    "        # Build array versions of the columns\n",
    "        tmp = df\n",
    "        arr_names = []\n",
    "        for c in cols_to_zip:\n",
    "            # Cast to string, split, then clean\n",
    "            arr = F.split(F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")), F.lit(sep_map[c]))\n",
    "            if trim_tokens:\n",
    "                arr = F.transform(arr, lambda x: F.trim(x))\n",
    "            if drop_empty_tokens:\n",
    "                arr = F.filter(arr, lambda x: x != \"\")\n",
    "            arr_name = f\"__{c}__arr\"\n",
    "            tmp = tmp.withColumn(arr_name, arr)\n",
    "            arr_names.append(arr_name)\n",
    "\n",
    "        # In strict mode, keep only rows where all arrays have equal size\n",
    "        if mode == \"strict\":\n",
    "            size0 = F.size(F.col(arr_names[0]))\n",
    "            cond = F.lit(True)\n",
    "            for a in arr_names[1:]:\n",
    "                cond = cond & (F.size(F.col(a)) == size0)\n",
    "            tmp = tmp.filter(cond)\n",
    "\n",
    "        # Zip arrays so positions are aligned; explode rows\n",
    "        zipped = F.arrays_zip(*[F.col(a) for a in arr_names]).alias(\"__zipped\")\n",
    "        exploded = tmp.withColumn(\"__z\", F.explode(zipped)).drop(\"__zipped\")\n",
    "\n",
    "        # Select original non-exploded columns (if requested)\n",
    "        base_cols = [c for c in df.columns if (c not in cols_to_zip)] if keep_other_cols else []\n",
    "\n",
    "        # Pull aligned values back out of the exploded struct and rename to original col names\n",
    "        select_exprs = [F.col(c) for c in base_cols]\n",
    "        for c in cols_to_zip:\n",
    "            select_exprs.append(F.col(f\"__z.__{c}__arr\").alias(c))\n",
    "\n",
    "        out = exploded.select(*select_exprs)\n",
    "\n",
    "        # Shortest mode: drop rows where any aligned value is NULL\n",
    "        if mode == \"shortest\":\n",
    "            cond = F.lit(True)\n",
    "            for c in cols_to_zip:\n",
    "                cond = cond & F.col(c).isNotNull()\n",
    "                out = out.filter(cond)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def clean_data(self, raw_folder) -> DataFrame:\n",
    "        latest_file = self.get_latest_file_in_folder(raw_folder, \"*.csv\")\n",
    "        print(f\"Cleaning latest file: {latest_file}\")\n",
    "\n",
    "        df = self.spark.read.option(\"header\", True).csv(latest_file)\n",
    "\n",
    "        #  Explode the columns\n",
    "        from pyspark.sql.functions import split, explode_outer\n",
    "\n",
    "        exploded_df = self.explode_zip_columns(\n",
    "            df,\n",
    "            cols_to_zip=[\"user_id\", \"user_name\", \"review_id\", \"review_title\", \"review_content\"],\n",
    "            sep=\",\",\n",
    "            mode=\"longest\"\n",
    "        )\n",
    "        pdf = exploded_df.toPandas()\n",
    "\n",
    "        # Remove HTML tags and links using BeautifulSoup in Pandas\n",
    "        \n",
    "\n",
    "        def clean_html(text):\n",
    "            if pd.isnull(text):\n",
    "                return None\n",
    "            # Only treat as HTML if it contains HTML tags\n",
    "            if re.search(r\"<.*?>\", str(text)):\n",
    "                soup = BeautifulSoup(text, \"lxml\")\n",
    "                for a in soup.find_all('a'):\n",
    "                    a.decompose()\n",
    "                return soup.get_text(separator=\" \", strip=True).lower().strip()\n",
    "            else:\n",
    "                return str(text).lower().strip()\n",
    "\n",
    "        pdf[\"review_content\"] = pdf[\"review_content\"].apply(clean_html)\n",
    "        pdf[\"review_title\"] = pdf[\"review_title\"].apply(clean_html)\n",
    "\n",
    "\n",
    "        pdf[\"user_name\"] = pdf[\"user_name\"].str.strip().str.lower()\n",
    "        \n",
    "        # lowercase the columns\n",
    "        pdf[\"review_id\"] = pdf[\"review_id\"].str.strip().str.lower()\n",
    "        pdf[\"user_id\"] = pdf[\"user_id\"].str.strip().str.lower()\n",
    "        \n",
    "        # Drop rows where review_content became null or empty after cleaning\n",
    "        # display(pdf[pdf.isnull().any(axis=1)])\n",
    "        # print(pdf.count())\n",
    "        pdf = pdf[pdf[\"review_content\"].notnull() & (pdf[\"review_content\"].str.strip() != \"\")]\n",
    "\n",
    "\n",
    "        # Suspicious review detection in Pandas\n",
    "        pdf[\"word_count\"] = pdf[\"review_content\"].str.split().str.len()\n",
    "        pdf[\"distinct_words_count\"] = pdf[\"review_content\"].str.split().apply(lambda x: len(set(x)) if isinstance(x, list) else 0)\n",
    "        pdf[\"is_suspicious_length\"] = pdf[\"word_count\"] < 5\n",
    "        pdf[\"is_suspicious_repetitive\"] = pdf[\"distinct_words_count\"] == 1\n",
    "        pdf[\"suspicious_review\"] = pdf[\"is_suspicious_length\"] | pdf[\"is_suspicious_repetitive\"]\n",
    "        pdf = pdf.drop(columns=[\"word_count\", \"distinct_words_count\", \"is_suspicious_length\", \"is_suspicious_repetitive\"])\n",
    "\n",
    "        # Convert back to Spark DataFrame\n",
    "        df = self.spark.createDataFrame(pdf)\n",
    "        # print(df.count())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform the input DataFrame.\n",
    "        \"\"\"\n",
    "        df = self.map_sentiment_spark(df, rating_col=\"rating\")\n",
    "        df_final = df.select(\"review_content\", \"sentiment\")\n",
    "        return df_final\n",
    "\n",
    "    def save_output(self, df, output_folder, output_filename=\"training_dataset.csv\"):\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "        print(\"This is the CSV file we are creating.\")\n",
    "        print(f\"Output saved to {output_path}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            # Step 1: Ingest latest file (optional)\n",
    "            self.ingest_latest_file(self.input_folder, self.raw_folder)\n",
    "\n",
    "            # Step 2: Clean data\n",
    "            df = self.clean_data(self.raw_folder)\n",
    "            # df.show(5, truncate=False) # show records\n",
    "            # display(df)\n",
    "\n",
    "            # Step 3: Transform data\n",
    "            df_final = self.transform(df)\n",
    "            # df_final.show(5, truncate=False)\n",
    "            # display(df_final)\n",
    "\n",
    "            # Step 4: Save output\n",
    "            self.save_output(df_final, self.output_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"review_dataset\").getOrCreate()\n",
    "    task = ReviewDataset(spark, input_file_path, raw_file_path, output_file_path)\n",
    "    task.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dc918d-e4bd-453a-9420-18624410751d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import array_distinct, size, split\n",
    "\n",
    "class ReviewDataset:\n",
    "    def __init__(self, spark, input_folder, raw_folder, output_folder):\n",
    "        self.input_folder = input_folder\n",
    "        self.raw_folder = raw_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.spark = spark\n",
    "\n",
    "    @staticmethod\n",
    "    def get_latest_file_in_folder(folder_path: str, pattern: str = \"*.csv\") -> str:\n",
    "        files = glob.glob(os.path.join(folder_path, pattern))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found in {folder_path} with pattern {pattern}\")\n",
    "        latest_file = max(files, key=os.path.getmtime)\n",
    "        return latest_file\n",
    "\n",
    "    @staticmethod\n",
    "    def move_file_to_folder(src_file_path: str, dest_folder: str) -> str:\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        filename = os.path.basename(src_file_path)\n",
    "        dest_file_path = os.path.join(dest_folder, filename)\n",
    "        shutil.move(src_file_path, dest_file_path)\n",
    "        print(f\"Moved file {filename} to {dest_folder}\")\n",
    "        return dest_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_html(text: str):\n",
    "        from bs4 import BeautifulSoup\n",
    "        if text is None:\n",
    "            return None\n",
    "        return BeautifulSoup(text, \"lxml\").get_text()\n",
    "\n",
    "    @staticmethod\n",
    "    def spark_clean_text_udf():\n",
    "        def clean_text(text):\n",
    "            if text is None or text.strip() == \"\":\n",
    "                return None\n",
    "            from bs4 import BeautifulSoup\n",
    "            cleaned = ReviewDataset.remove_html(text)\n",
    "            if cleaned is not None:\n",
    "                return cleaned.lower().strip()\n",
    "            else:\n",
    "                return None\n",
    "        return udf(clean_text, StringType())\n",
    "\n",
    "    @staticmethod\n",
    "    def spark_map_sentiment_udf():\n",
    "        def map_sentiment(rating):\n",
    "            try:\n",
    "                r = float(rating)\n",
    "            except Exception:\n",
    "                return \"neutral\"\n",
    "            if r >= 4.0:\n",
    "                return \"positive\"\n",
    "            elif r == 3.0:\n",
    "                return \"neutral\"\n",
    "            elif r <= 2.0:\n",
    "                return \"negative\"\n",
    "            else:\n",
    "                return \"neutral\"\n",
    "        return udf(map_sentiment, StringType())\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_suspicious_reviews_spark(df, review_col: str = \"review_content\", min_word_len: int = 5):\n",
    "        df = df.withColumn(\"word_count\", size(split(col(review_col), \" \")))\n",
    "        df = df.withColumn(\"is_suspicious_length\", col(\"word_count\") < min_word_len)\n",
    "        df = df.withColumn(\"distinct_words_count\", size(array_distinct(split(col(review_col), \" \"))))\n",
    "        df = df.withColumn(\"is_suspicious_repetitive\", col(\"distinct_words_count\") == 1)\n",
    "        df = df.withColumn(\"suspicious_review\", col(\"is_suspicious_length\") | col(\"is_suspicious_repetitive\"))\n",
    "        df = df.drop(\"word_count\", \"distinct_words_count\", \"is_suspicious_length\", \"is_suspicious_repetitive\")\n",
    "        return df\n",
    "\n",
    "    def ingest_latest_file(self, input_folder, output_folder):\n",
    "        latest_file = self.get_latest_file_in_folder(input_folder, \"*.csv\")\n",
    "        moved_file = self.move_file_to_folder(latest_file, output_folder)\n",
    "        print(f\"Ingested file is now at: {moved_file}\")\n",
    "        return moved_file\n",
    "\n",
    "    def clean_data(self, raw_folder):\n",
    "        latest_file = self.get_latest_file_in_folder(raw_folder, \"*.csv\")\n",
    "        print(f\"Cleaning latest file: {latest_file}\")\n",
    "        df = self.spark.read.option(\"header\", True).csv(latest_file)\n",
    "        clean_text_udf = self.spark_clean_text_udf()\n",
    "        df = df.withColumn(\"review_content\", clean_text_udf(\"review_content\"))\n",
    "        df = df.na.drop(subset=[\"review_content\"])\n",
    "        df = self.detect_suspicious_reviews_spark(df, review_col=\"review_content\")\n",
    "        # display(df)\n",
    "        return df\n",
    "\n",
    "    def transform(self, df):\n",
    "        map_sent_udf = self.spark_map_sentiment_udf()\n",
    "        df = df.withColumn(\"sentiment\", map_sent_udf(\"rating\"))\n",
    "        df_final = df.select(\"review_content\", \"sentiment\")\n",
    "        return df_final\n",
    "\n",
    "    def save_output(self, df, output_folder, output_filename=\"training_dataset.csv\"):\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "        print(f\"Output saved to {output_path}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        try:\n",
    "            # Step 1: Ingest latest file\n",
    "            # self.ingest_latest_file(self.input_folder, self.output_folder)\n",
    "\n",
    "            # Step 2: Clean data\n",
    "            df = self.clean_data(self.raw_folder)\n",
    "            df.printSchema()\n",
    "            display(df) # Already displayed in clean_data\n",
    "\n",
    "            # Step 3: Transform data\n",
    "            df_final = self.transform(df)\n",
    "            display(df_final)\n",
    "\n",
    "            # Step 4: Save output\n",
    "            self.save_output(df_final, self.output_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"review_dataset\").getOrCreate()\n",
    "    task = ReviewDataset(spark, input_file_path, raw_file_path, output_file_path)\n",
    "    task.run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "end_to_end_pipeline",
   "widgets": {
    "input_file_path": {
     "currentValue": "/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/pre_raw/",
     "nuid": "d63b4289-c095-4acd-b2e0-6323425e16b3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "input_file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "input_file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "output_file_path": {
     "currentValue": "/Workspace/Users/Groups/",
     "nuid": "2ad225c1-f375-498b-b550-83dd35efab92",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "output_file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "output_file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "raw_file_path": {
     "currentValue": "/Workspace/Users/rojsun25@gmail.com/messy_review_dataset_model/raw/",
     "nuid": "d8515c47-d4ce-4418-8838-148013309f17",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "raw_file_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "raw_file_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
